groups:
- name: "nodes"
  rules:
    - alert: "CephNodeRootFilesystemFull"
      annotations:
        description: "Root volume is dangerously full: {{ $value | humanize }}% free."
        summary: "Root filesystem is dangerously full"
      expr: "node_filesystem_avail_bytes{mountpoint=\"/\"} / node_filesystem_size_bytes{mountpoint=\"/\"} * 100 < 5"
      for: "5m"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.8.1"
        severity: "critical"
        type: "ceph_default"
        service: ceph
        #support_group: storage
    - alert: "CephNodeNetworkPacketDrops"
      annotations:
        description: "Node {{ $labels.instance }} experiences packet drop > 0.5% or > 10 packets/s on interface {{ $labels.device }}."
        summary: "One or more NICs reports packet drops"
      expr: |
        (
          rate(node_network_receive_drop_total{device="bond0"}[1m]) +
          rate(node_network_transmit_drop_total{device="bond0"}[1m])
        ) / (
          rate(node_network_receive_packets_total{device="bond0"}[1m]) +
          rate(node_network_transmit_packets_total{device="bond0"}[1m])
        ) >= 0.0050000000000000001 and (
          rate(node_network_receive_drop_total{device="bond0"}[1m]) +
          rate(node_network_transmit_drop_total{device="bond0"}[1m])
        ) >= 10
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.8.2"
        severity: "warning"
        type: "ceph_default"
        service: ceph
        #support_group: storage
    - alert: "CephNodeNetworkPacketErrors"
      annotations:
        description: "Node {{ $labels.instance }} experiences packet errors > 0.01% or > 10 packets/s on interface {{ $labels.device }}."
        summary: "One or more NICs reports packet errors"
      expr: |
        (
          rate(node_network_receive_errs_total{device="bond0"}[1m]) +
          rate(node_network_transmit_errs_total{device="bond0"}[1m])
        ) / (
          rate(node_network_receive_packets_total{device="bond0"}[1m]) +
          rate(node_network_transmit_packets_total{device="bond0"}[1m])
        ) >= 0.0001 or (
          rate(node_network_receive_errs_total{device="bond0"}[1m]) +
          rate(node_network_transmit_errs_total{device="bond0"}[1m])
        ) >= 10
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.8.3"
        severity: "warning"
        type: "ceph_default"
        service: ceph
        #support_group: storage
    - alert: "CephNodeNetworkBondDegraded"
      annotations:
        summary: "Degraded Bond on Node {{ $labels.instance }}"
        description: "Bond {{ $labels.master }} is degraded on Node {{ $labels.instance }}."
      expr: |
        node_bonding_slaves - node_bonding_active != 0
      labels:
        severity: "warning"
        type: "ceph_default"
        service: ceph
        #support_group: storage
    - alert: "CephNodeExporterMissing"
      annotations:
        summary: "Node exporter is not running on Node {{ $labels.node }}"
        description: "Node exporter is not running on Node {{ $labels.node }}. Check the node exporter pod and network status of the node."
      expr: |
        up{job="node-exporter"} == 0
      labels:
        severity: "warning"
        type: "ceph_default"
        service: ceph
        #support_group: storage
    - alert: "CephNodeDiskspaceWarning"
      annotations:
        description: "Mountpoint {{ $labels.mountpoint }} on {{ $labels.nodename }} will be full in less than 5 days based on the 48 hour trailing fill rate."
        summary: "Host filesystem free space is getting low"
      expr: "predict_linear(node_filesystem_free_bytes{device=~\"/.*\", mountpoint!=\"/boot\"}[2d], 3600 * 24 * 5) *on(instance) group_left(nodename) node_uname_info < 0"
      labels:
        oid: "1.3.6.1.4.1.50495.1.2.1.8.4"
        severity: "warning"
        type: "ceph_default"
        service: ceph
        #support_group: storage
    # - alert: "CephNodeInconsistentMTU"
    #   annotations:
    #     description: "Node {{ $labels.instance }} has a different MTU size ({{ $value }}) than the median of devices named {{ $labels.device }}."
    #     summary: "MTU settings across Ceph hosts are inconsistent"
    #   expr: "node_network_mtu_bytes * (node_network_up{device!=\"lo\"} > 0) ==  scalar(    max by (device) (node_network_mtu_bytes * (node_network_up{device!=\"lo\"} > 0)) !=      quantile by (device) (.5, node_network_mtu_bytes * (node_network_up{device!=\"lo\"} > 0))  )or node_network_mtu_bytes * (node_network_up{device!=\"lo\"} > 0) ==  scalar(    min by (device) (node_network_mtu_bytes * (node_network_up{device!=\"lo\"} > 0)) !=      quantile by (device) (.5, node_network_mtu_bytes * (node_network_up{device!=\"lo\"} > 0))  )"
    #   labels:
    #     severity: "warning"
    #     type: "ceph_default"
    #     service: ceph
    #     support_group: storage
